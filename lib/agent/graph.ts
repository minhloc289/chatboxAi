import { AIMessage } from "@langchain/core/messages";
import { RunnableConfig } from "@langchain/core/runnables";
import { END, START, MessagesAnnotation, StateGraph } from "@langchain/langgraph";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { getPromptTemplate } from "../agent/configuration"; // ƒë·∫£m b·∫£o b·∫°n export h√†m n√†y t·ª´ configuration.ts

import { ConfigurationSchema, ensureConfiguration } from "../agent/configuration";
import { TOOLS } from "../agent/tools";
import { CLASSIFY_INTENT_PROMPT } from "../agent/prompt";

// H√†m ph√¢n lo·∫°i intent t·ª´ user query
async function classifyIntent(config: RunnableConfig): Promise<string> {
  const userQuery = config?.configurable?.userQuery || "";
  const model = new ChatGoogleGenerativeAI({
    model: "gemini-2.0-flash",
    temperature: 0,
  });

  

  const prompt = CLASSIFY_INTENT_PROMPT.replace("{user_query}", userQuery);
  const response = await model.invoke([{ role: "user", content: prompt }]);
  const intent = typeof response.content === "string" ? response.content.trim().toLowerCase() : "general";
  return ["general", "database", "search"].includes(intent) ? intent : "general";
}

// Node x·ª≠ l√Ω intent

async function classify_intent_node(
  state: typeof MessagesAnnotation.State,
  config: RunnableConfig,
): Promise<typeof MessagesAnnotation.Update> {
  const intent = await classifyIntent(config);
  const systemPromptTemplate = getPromptTemplate(intent);

  console.log(">>> Intent:", intent);
  console.log(">>> System Prompt:", systemPromptTemplate.slice(0, 100) + "...");


  config.configurable = {
    ...config.configurable,
    intent,
    systemPromptTemplate, // üëà c·∫≠p nh·∫≠t prompt t∆∞∆°ng ·ª©ng v√†o config
  };

  return { messages: [...state.messages] };
}


// G·ªçi model ch√≠nh ƒë·ªÉ tr·∫£ l·ªùi ng∆∞·ªùi d√πng
export async function callModel(
  state: typeof MessagesAnnotation.State,
  config: RunnableConfig,
): Promise<typeof MessagesAnnotation.Update> {
  const configuration = ensureConfiguration(config);
  const model = new ChatGoogleGenerativeAI({
    model: configuration.model,
    temperature: 0.7,
  }).bindTools(TOOLS);


  const response = await model.invoke([
    {
      role: "system",
      content: configuration.systemPromptTemplate,
    },
    ...state.messages,
  ]);

  return { messages: [response] };
}

// H√†m quy·∫øt ƒë·ªãnh chuy·ªÉn h∆∞·ªõng sau khi model tr·∫£ l·ªùi
function routeModelOutput(state: typeof MessagesAnnotation.State): string {
  const messages = state.messages;
  const lastMessage = messages[messages.length - 1];
  if ((lastMessage as AIMessage)?.tool_calls?.length || 0 > 0) {
    return "tools";
  } else {
    return END;
  }
}

function routeByIntent(_: typeof MessagesAnnotation.State, config: RunnableConfig): string {
  const intent = config.configurable?.intent || "general";

  if (intent === "database" || intent === "search") {
    return "tools"; // G·ªçi c√¥ng c·ª• t√¨m ki·∫øm ho·∫∑c truy v·∫•n DB
  }

  return "callModel"; // Ng∆∞·ª£c l·∫°i, x·ª≠ l√Ω b√¨nh th∆∞·ªùng b·∫±ng LLM
}


// Kh·ªüi t·∫°o workflow ch√≠nh
const workflow = new StateGraph(MessagesAnnotation, ConfigurationSchema)
  .addNode("classify_intent", classify_intent_node)
  .addNode("callModel", callModel)
  .addNode("tools", new ToolNode(TOOLS))
  .addEdge(START, "classify_intent")
  .addConditionalEdges("classify_intent", routeByIntent)
  .addConditionalEdges("callModel", routeModelOutput)
  .addEdge("tools", "callModel");


export const graph = workflow.compile({
  interruptBefore: [],
  interruptAfter: [],
});
